{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employment_Info_1 float64 19762.0\n",
      "Employment_Info_4 float64 17628.0\n",
      "Employment_Info_6 float64 15978.0\n",
      "Insurance_History_5 float64 11660.0\n",
      "Family_Hist_2 float64 9885.0\n",
      "Family_Hist_3 float64 8701.0\n",
      "Family_Hist_4 float64 13088.0\n",
      "Family_Hist_5 float64 6141.0\n",
      "Medical_History_1 float64 16793.0\n",
      "Medical_History_10 float64 201.0\n",
      "Medical_History_15 float64 4901.0\n",
      "Medical_History_24 float64 1180.0\n",
      "Medical_History_32 float64 351.0\n",
      "Medical_History_1 1.0\n",
      "Medical_History_10 240.0\n",
      "Medical_History_24 0.0\n",
      "Medical_History_32 0.0\n",
      "Medical_History_15 240.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#zf=zipfile.ZipFile('train.csv.zip')\n",
    "#zft=zipfile.ZipFile('test.csv.zip')\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "\n",
    "temp1=[]\n",
    "for i in test.columns:\n",
    "    if (test[i].describe()[0]) != 19765:\n",
    "        print (i,test[i].dtype,test[i].describe()[0])\n",
    "        temp1.append(i)\n",
    "\"\"\"#temp=[]\n",
    "for i in train.columns:\n",
    "    if (train[i].describe()[0]) != 59381:\n",
    "        print (i,train[i].dtype,train[i].describe()[0])\n",
    "        #temp.append(i)\n",
    "\"\"\"\n",
    "meds=['Medical_History_1','Medical_History_10','Medical_History_24','Medical_History_32','Medical_History_15']\n",
    "for i in meds:\n",
    "    temp1.remove(i)\n",
    "\n",
    "\n",
    "#train.drop('Insurance_History_5',axis=1,inplace=True)\n",
    "#test.drop('Medical_History_24',axis=1,inplace=True)\n",
    "#test.drop('Medical_History_32',axis=1,inplace=True)\n",
    "#train.drop('Medical_History_10',axis=1,inplace=True)\n",
    "#train.drop('Medical_History_24',axis=1,inplace=True)\n",
    "#train.drop('Medical_History_32',axis=1,inplace=True)\n",
    "#temp1.remove('Insurance_History_5')\n",
    "#temp1.remove('Medical_History_10')\n",
    "#temp.remove('Medical_History_10')\n",
    "#temp.remove('Insurance_History_5')\n",
    "#train.drop('Medical_History_10',axis=1,inplace=True)\n",
    "#test.drop('Medical_History_10',axis=1,inplace=True)\n",
    "\n",
    "for i in temp1:\n",
    "    men=train[i].mean()\n",
    "    test[i].fillna(men,inplace=True)\n",
    "    train[i].fillna(men,inplace=True)\n",
    "\n",
    "for i in meds:\n",
    "    mo=test[i].mode()[0]\n",
    "    print (i,mo)\n",
    "    test[i].fillna(mo,inplace=True)\n",
    "    train[i].fillna(mo,inplace=True)\n",
    "\n",
    "y=train.pop('Response')\n",
    "ids=test.pop('Id')\n",
    "train.drop('Id',axis=1,inplace=True)\n",
    "\n",
    "le=LabelEncoder()\n",
    "train['Product_Info_2']=le.fit_transform(train['Product_Info_2'])\n",
    "test['Product_Info_2']=le.fit_transform(test['Product_Info_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain=xgb.DMatrix(train,label=y)\n",
    "dtest=xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'bst:max_depth':9, 'bst:eta':0.3, 'bst:gamma':5,'silent':0, 'objective':'multi:softmax','num_class':9}\n",
    "param[\"colsample_bytree\"] = 0.70\n",
    "param[\"subsample\"] = 0.70\n",
    "param[\"min_child_weight\"] = 1\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] ='merror'\n",
    "plst=list(param.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'num_boost_round': [10, 25, 50],\n",
    "        'eta': [0.05, 0.1, 0.3],\n",
    "        'max_depth': [6, 9, 12],\n",
    "        'subsample': [0.9, 1.0],\n",
    "        'colsample_bytree': [0.9, 1.0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-45acbef876cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    660\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gemunu/Library/Python/3.4/lib/python/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/xgboost-0.4a30-py3.4.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    341\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/xgboost-0.4a30-py3.4.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mnboost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/xgboost-0.4a30-py3.4.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(xgbc, params).fit(train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': [0.9, 1.0],\n",
       " 'eta': [0.05, 0.1, 0.3],\n",
       " 'max_depth': [6, 9, 12],\n",
       " 'num_boost_round': [100, 250, 500],\n",
       " 'subsample': [0.9, 1.0]}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params={'colsample_bytree': [0.9, 1.0],\n",
    " 'learning_rate': [0.05, 0.1, 0.3],\n",
    " 'max_depth': [6, 9, 12],\n",
    " 'n_estimators': [100, 250, 500],\n",
    " 'subsample': [0.9, 1.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgbc=xgb.XGBClassifier(objective='multi:softmax',missing=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_wrapper(yhat, y):\n",
    "    y = np.array(y)\n",
    "    y = y.astype(int)\n",
    "    yhat = np.array(yhat)\n",
    "    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)\n",
    "    return quadratic_weighted_kappa(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'eta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-606e2dd1a3eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bst=xgb.train(plst,dtrain,num_boost_round=97,eta=.1)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'eta'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bst=xgb.train(plst,dtrain,num_boost_round=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_pred=bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_predp=bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_pred=bst_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 10 rounds.\n",
      "[0]\tcv-test-merror:0.4645763333333333+0.00525364389944943\tcv-train-merror:0.43316033333333337+0.005598986237604888\n",
      "[1]\tcv-test-merror:0.449706+0.004246796675142328\tcv-train-merror:0.41344866666666663+0.0064408312265490115\n",
      "[2]\tcv-test-merror:0.4419596666666667+0.0031394813088930712\tcv-train-merror:0.40340333333333334+0.008056838351501304\n",
      "[3]\tcv-test-merror:0.43875966666666666+0.0030794289874729822\tcv-train-merror:0.3981323333333333+0.007506012227245281\n",
      "[4]\tcv-test-merror:0.436335+0.003705690848771198\tcv-train-merror:0.394916+0.00828557650378054\n",
      "[5]\tcv-test-merror:0.4331853333333333+0.001412162486716343\tcv-train-merror:0.39067233333333334+0.0059714981555906155\n",
      "[6]\tcv-test-merror:0.4306933333333333+0.0002806294512143815\tcv-train-merror:0.38596499999999995+0.0044025929481007735\n",
      "[7]\tcv-test-merror:0.428706+0.0012077259622944182\tcv-train-merror:0.38407066666666667+0.0033419754969511856\n",
      "[8]\tcv-test-merror:0.42841966666666664+0.0009053402061594795\tcv-train-merror:0.3817886666666667+0.0026388231383621574\n",
      "[9]\tcv-test-merror:0.4283523333333333+0.0017156661161841014\tcv-train-merror:0.37994500000000003+0.002979607468554656\n",
      "[10]\tcv-test-merror:0.42737600000000003+0.002276998609280807\tcv-train-merror:0.3783026666666667+0.0025268955041490883\n",
      "[11]\tcv-test-merror:0.4265843333333334+0.0019964141743525118\tcv-train-merror:0.37703099999999995+0.002521632936544622\n",
      "[12]\tcv-test-merror:0.4257253333333333+0.0024221467521372076\tcv-train-merror:0.37540633333333334+0.0025178055701123624\n",
      "[13]\tcv-test-merror:0.425675+0.002755383457887485\tcv-train-merror:0.3735456666666666+0.0024020452859085653\n",
      "[14]\tcv-test-merror:0.42564133333333337+0.0029805655317219426\tcv-train-merror:0.3724593333333333+0.0027121466692558343\n",
      "[15]\tcv-test-merror:0.42523700000000003+0.0030924198723114465\tcv-train-merror:0.3710613333333333+0.0026421997569365608\n",
      "[16]\tcv-test-merror:0.4247483333333333+0.0029273077885471565\tcv-train-merror:0.36986566666666665+0.0027180508130316937\n",
      "[17]\tcv-test-merror:0.4242436666666667+0.0030986536144303437\tcv-train-merror:0.3687123333333333+0.0028483511409624737\n",
      "[18]\tcv-test-merror:0.4240916666666667+0.003319611456514498\tcv-train-merror:0.36769333333333326+0.002591545399092129\n",
      "[19]\tcv-test-merror:0.4234183333333334+0.003267882324822735\tcv-train-merror:0.3666406666666666+0.0026576521384276126\n",
      "[20]\tcv-test-merror:0.423452+0.003209068816131338\tcv-train-merror:0.36574799999999996+0.0026107586381484476\n",
      "[21]\tcv-test-merror:0.42316533333333334+0.0030160577285957817\tcv-train-merror:0.365083+0.0028835382200807945\n",
      "[22]\tcv-test-merror:0.42309833333333335+0.0034566141751077085\tcv-train-merror:0.3643+0.0027145577908749803\n",
      "[23]\tcv-test-merror:0.42335100000000003+0.002961280916540465\tcv-train-merror:0.36323900000000003+0.00337553146432776\n",
      "[24]\tcv-test-merror:0.4226096666666667+0.003139076863594699\tcv-train-merror:0.362439+0.0032764251860831505\n",
      "[25]\tcv-test-merror:0.42240800000000006+0.002967727188719772\tcv-train-merror:0.3615716666666667+0.003509616344971194\n",
      "[26]\tcv-test-merror:0.4220373333333334+0.0029889213587662175\tcv-train-merror:0.3607466666666667+0.0034829218895761653\n",
      "[27]\tcv-test-merror:0.4216166666666667+0.003178115304530167\tcv-train-merror:0.360233+0.003368233166909132\n",
      "[28]\tcv-test-merror:0.42102700000000004+0.0032143740707432757\tcv-train-merror:0.35947500000000004+0.0034530341247469174\n",
      "[29]\tcv-test-merror:0.42102700000000004+0.0031893768461357273\tcv-train-merror:0.3587846666666667+0.0035405152481273504\n",
      "[30]\tcv-test-merror:0.4208416666666667+0.003169655116184635\tcv-train-merror:0.35801+0.003061106444844198\n",
      "[31]\tcv-test-merror:0.4209766666666666+0.003478644308092203\tcv-train-merror:0.357581+0.0029573448677262257\n",
      "[32]\tcv-test-merror:0.42033666666666664+0.0035976179464875012\tcv-train-merror:0.3568733333333333+0.002687908893958681\n",
      "[33]\tcv-test-merror:0.42016800000000004+0.0033552041169899994\tcv-train-merror:0.356208+0.0023740468964758643\n",
      "[34]\tcv-test-merror:0.4204543333333333+0.0033351328142802346\tcv-train-merror:0.35601466666666665+0.002683374161180079\n",
      "[35]\tcv-test-merror:0.420421+0.0035835045230425936\tcv-train-merror:0.3554+0.0025776562998196676\n",
      "[36]\tcv-test-merror:0.42020166666666664+0.0037627866742024536\tcv-train-merror:0.3544316666666667+0.0026522952743279244\n",
      "[37]\tcv-test-merror:0.4204206666666666+0.003177021386281313\tcv-train-merror:0.353783+0.0025794822477905573\n",
      "[38]\tcv-test-merror:0.42053866666666667+0.003604184728278438\tcv-train-merror:0.3531516666666667+0.0026459143767115635\n",
      "[39]\tcv-test-merror:0.42057266666666665+0.0038970539585121942\tcv-train-merror:0.3526126666666667+0.0025884339323657035\n",
      "[40]\tcv-test-merror:0.42072400000000004+0.003894128229355909\tcv-train-merror:0.352133+0.0026104560265721114\n",
      "[41]\tcv-test-merror:0.4202863333333333+0.004301402122822529\tcv-train-merror:0.351855+0.0022004126885654765\n",
      "[42]\tcv-test-merror:0.4203029999999999+0.004466697288452248\tcv-train-merror:0.3509793333333333+0.0022672221378202523\n",
      "[43]\tcv-test-merror:0.420202+0.00469349422072724\tcv-train-merror:0.350255+0.002230689579479862\n",
      "Stopping. Best iteration: 33\n"
     ]
    }
   ],
   "source": [
    "res=xgb.cv(param,dtrain,num_boost_round=200,show_progress=True,early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-merror-mean</th>\n",
       "      <th>test-merror-std</th>\n",
       "      <th>train-merror-mean</th>\n",
       "      <th>train-merror-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.473350</td>\n",
       "      <td>0.006502</td>\n",
       "      <td>0.465317</td>\n",
       "      <td>0.006066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.461629</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.450851</td>\n",
       "      <td>0.008531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.456543</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>0.445446</td>\n",
       "      <td>0.007563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.450043</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.438987</td>\n",
       "      <td>0.007263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.447719</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.435973</td>\n",
       "      <td>0.006184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.447954</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.004698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.443004</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.429195</td>\n",
       "      <td>0.003216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.440444</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.425936</td>\n",
       "      <td>0.002035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.439635</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.423780</td>\n",
       "      <td>0.002286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.438591</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.421852</td>\n",
       "      <td>0.002247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.437160</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.419882</td>\n",
       "      <td>0.002267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.435577</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.417634</td>\n",
       "      <td>0.003046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.434853</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.416556</td>\n",
       "      <td>0.003022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.433826</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.415368</td>\n",
       "      <td>0.002977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.432984</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.414459</td>\n",
       "      <td>0.002778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.432495</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.413180</td>\n",
       "      <td>0.003054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.431704</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.411925</td>\n",
       "      <td>0.003223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.430744</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.411420</td>\n",
       "      <td>0.003369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.430828</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.410232</td>\n",
       "      <td>0.003249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.429834</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.408835</td>\n",
       "      <td>0.003142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.429683</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.407866</td>\n",
       "      <td>0.003211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.429195</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.406755</td>\n",
       "      <td>0.003039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.429110</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.405618</td>\n",
       "      <td>0.003151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.428454</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.404734</td>\n",
       "      <td>0.003322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.428016</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.403471</td>\n",
       "      <td>0.002884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.427308</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.402848</td>\n",
       "      <td>0.002810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.426618</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.002711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.425827</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.400953</td>\n",
       "      <td>0.002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.425321</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.400322</td>\n",
       "      <td>0.002429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.425405</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>0.002133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.422677</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.394167</td>\n",
       "      <td>0.003432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.422627</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.393661</td>\n",
       "      <td>0.003388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.422728</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.393156</td>\n",
       "      <td>0.003105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.422694</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.392929</td>\n",
       "      <td>0.003422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.422189</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>0.392289</td>\n",
       "      <td>0.003578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.422543</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.392137</td>\n",
       "      <td>0.003499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.422576</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.391750</td>\n",
       "      <td>0.003562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.422492</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.391455</td>\n",
       "      <td>0.003515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.422425</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.391270</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.422189</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.391085</td>\n",
       "      <td>0.003607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.422357</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.390478</td>\n",
       "      <td>0.003902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.422189</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0.390251</td>\n",
       "      <td>0.003455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.422004</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>0.389948</td>\n",
       "      <td>0.003225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.421785</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.389527</td>\n",
       "      <td>0.003318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.421751</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.389224</td>\n",
       "      <td>0.003383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.421684</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.388853</td>\n",
       "      <td>0.003191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.421212</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.388508</td>\n",
       "      <td>0.003111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.421532</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.387868</td>\n",
       "      <td>0.002995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.421145</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.387725</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.421684</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.387464</td>\n",
       "      <td>0.003087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.421684</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.387110</td>\n",
       "      <td>0.003088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.421465</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.386790</td>\n",
       "      <td>0.003182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.421078</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.386209</td>\n",
       "      <td>0.003131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.421078</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.386066</td>\n",
       "      <td>0.003151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.420892</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.385940</td>\n",
       "      <td>0.003148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.420859</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.385611</td>\n",
       "      <td>0.003017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.420589</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.385334</td>\n",
       "      <td>0.002962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.420303</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.385350</td>\n",
       "      <td>0.003255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.420353</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.384820</td>\n",
       "      <td>0.003361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.420067</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.384475</td>\n",
       "      <td>0.003142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-merror-mean  test-merror-std  train-merror-mean  train-merror-std\n",
       "0           0.473350         0.006502           0.465317          0.006066\n",
       "1           0.461629         0.008163           0.450851          0.008531\n",
       "2           0.456543         0.005884           0.445446          0.007563\n",
       "3           0.450043         0.004437           0.438987          0.007263\n",
       "4           0.447719         0.002658           0.435973          0.006184\n",
       "5           0.447954         0.003002           0.434154          0.004698\n",
       "6           0.443004         0.001365           0.429195          0.003216\n",
       "7           0.440444         0.000547           0.425936          0.002035\n",
       "8           0.439635         0.000536           0.423780          0.002286\n",
       "9           0.438591         0.000927           0.421852          0.002247\n",
       "10          0.437160         0.001271           0.419882          0.002267\n",
       "11          0.435577         0.000573           0.417634          0.003046\n",
       "12          0.434853         0.001177           0.416556          0.003022\n",
       "13          0.433826         0.000756           0.415368          0.002977\n",
       "14          0.432984         0.000877           0.414459          0.002778\n",
       "15          0.432495         0.001281           0.413180          0.003054\n",
       "16          0.431704         0.001236           0.411925          0.003223\n",
       "17          0.430744         0.001191           0.411420          0.003369\n",
       "18          0.430828         0.001280           0.410232          0.003249\n",
       "19          0.429834         0.001339           0.408835          0.003142\n",
       "20          0.429683         0.000908           0.407866          0.003211\n",
       "21          0.429195         0.001019           0.406755          0.003039\n",
       "22          0.429110         0.001750           0.405618          0.003151\n",
       "23          0.428454         0.001500           0.404734          0.003322\n",
       "24          0.428016         0.002013           0.403471          0.002884\n",
       "25          0.427308         0.001817           0.402848          0.002810\n",
       "26          0.426618         0.001831           0.402174          0.002711\n",
       "27          0.425827         0.002209           0.400953          0.002365\n",
       "28          0.425321         0.002124           0.400322          0.002429\n",
       "29          0.425405         0.001701           0.399800          0.002133\n",
       "..               ...              ...                ...               ...\n",
       "39          0.422677         0.002241           0.394167          0.003432\n",
       "40          0.422627         0.001967           0.393661          0.003388\n",
       "41          0.422728         0.002556           0.393156          0.003105\n",
       "42          0.422694         0.002715           0.392929          0.003422\n",
       "43          0.422189         0.002466           0.392289          0.003578\n",
       "44          0.422543         0.002906           0.392137          0.003499\n",
       "45          0.422576         0.002687           0.391750          0.003562\n",
       "46          0.422492         0.002473           0.391455          0.003515\n",
       "47          0.422425         0.002738           0.391270          0.003609\n",
       "48          0.422189         0.002725           0.391085          0.003607\n",
       "49          0.422357         0.002488           0.390478          0.003902\n",
       "50          0.422189         0.002432           0.390251          0.003455\n",
       "51          0.422004         0.002856           0.389948          0.003225\n",
       "52          0.421785         0.002801           0.389527          0.003318\n",
       "53          0.421751         0.002609           0.389224          0.003383\n",
       "54          0.421684         0.002815           0.388853          0.003191\n",
       "55          0.421212         0.002779           0.388508          0.003111\n",
       "56          0.421532         0.002693           0.387868          0.002995\n",
       "57          0.421145         0.002454           0.387725          0.003101\n",
       "58          0.421684         0.002693           0.387464          0.003087\n",
       "59          0.421684         0.002733           0.387110          0.003088\n",
       "60          0.421465         0.003007           0.386790          0.003182\n",
       "61          0.421078         0.003231           0.386209          0.003131\n",
       "62          0.421078         0.003025           0.386066          0.003151\n",
       "63          0.420892         0.002593           0.385940          0.003148\n",
       "64          0.420859         0.002785           0.385611          0.003017\n",
       "65          0.420589         0.002877           0.385334          0.002962\n",
       "66          0.420303         0.002985           0.385350          0.003255\n",
       "67          0.420353         0.002877           0.384820          0.003361\n",
       "68          0.420067         0.002921           0.384475          0.003142\n",
       "\n",
       "[69 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_predt=bst.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5639877532379546"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_wrapper(y,bst_predt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resps=bst_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=[i for i,j in zip(bst_pred,resps) if i!=j]\n",
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(bst_pred)):\n",
    "    bst_pred[i].pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=9, max_features=0.33, max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc=RandomForestClassifier(n_estimators=600,max_depth=9,criterion='entropy',max_features=0.33)\n",
    "rfc.fit(train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_pred=rfc.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=3, max_features=0.33, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc=GradientBoostingClassifier(n_estimators=100,max_features=0.33)\n",
    "bc.fit(train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bc_pred=bc.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resps=[]\n",
    "for i in range(len(bst_pred)):\n",
    "    max_p=[]\n",
    "    for j in range(8):\n",
    "        max_p.append((0.4*bst_predp[i][j]+0.3*bc_pred[i][j])+0.3*rfc_pred[i][j]/3)\n",
    "    #print (max_p)\n",
    "    resps.append(max_p.index(max(max_p))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>59</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>69</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>83</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>86</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>89</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>92</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19735</th>\n",
       "      <td>79004</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19736</th>\n",
       "      <td>79007</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19737</th>\n",
       "      <td>79020</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19738</th>\n",
       "      <td>79022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19739</th>\n",
       "      <td>79027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19740</th>\n",
       "      <td>79028</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19741</th>\n",
       "      <td>79031</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19742</th>\n",
       "      <td>79035</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19743</th>\n",
       "      <td>79038</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19744</th>\n",
       "      <td>79047</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745</th>\n",
       "      <td>79048</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>79051</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19747</th>\n",
       "      <td>79054</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19748</th>\n",
       "      <td>79060</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19749</th>\n",
       "      <td>79064</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19750</th>\n",
       "      <td>79065</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19751</th>\n",
       "      <td>79067</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19752</th>\n",
       "      <td>79071</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19753</th>\n",
       "      <td>79072</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19754</th>\n",
       "      <td>79073</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19755</th>\n",
       "      <td>79080</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19756</th>\n",
       "      <td>79083</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19757</th>\n",
       "      <td>79084</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19758</th>\n",
       "      <td>79085</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19759</th>\n",
       "      <td>79089</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19760</th>\n",
       "      <td>79093</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19761</th>\n",
       "      <td>79099</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19762</th>\n",
       "      <td>79102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19763</th>\n",
       "      <td>79125</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19764</th>\n",
       "      <td>79129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19765 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Response\n",
       "0          1         2\n",
       "1          3         8\n",
       "2          4         6\n",
       "3          9         8\n",
       "4         12         8\n",
       "5         13         8\n",
       "6         21         8\n",
       "7         28         8\n",
       "8         30         7\n",
       "9         36         8\n",
       "10        38         8\n",
       "11        43         8\n",
       "12        45         4\n",
       "13        48         8\n",
       "14        50         6\n",
       "15        51         8\n",
       "16        54         7\n",
       "17        55         8\n",
       "18        59         8\n",
       "19        62         2\n",
       "20        63         8\n",
       "21        66         8\n",
       "22        69         8\n",
       "23        82         8\n",
       "24        83         6\n",
       "25        84         7\n",
       "26        86         8\n",
       "27        89         8\n",
       "28        90         1\n",
       "29        92         8\n",
       "...      ...       ...\n",
       "19735  79004         7\n",
       "19736  79007         8\n",
       "19737  79020         2\n",
       "19738  79022         1\n",
       "19739  79027         1\n",
       "19740  79028         8\n",
       "19741  79031         8\n",
       "19742  79035         2\n",
       "19743  79038         8\n",
       "19744  79047         6\n",
       "19745  79048         5\n",
       "19746  79051         6\n",
       "19747  79054         5\n",
       "19748  79060         6\n",
       "19749  79064         2\n",
       "19750  79065         5\n",
       "19751  79067         8\n",
       "19752  79071         2\n",
       "19753  79072         6\n",
       "19754  79073         8\n",
       "19755  79080         6\n",
       "19756  79083         2\n",
       "19757  79084         8\n",
       "19758  79085         6\n",
       "19759  79089         8\n",
       "19760  79093         8\n",
       "19761  79099         8\n",
       "19762  79102         1\n",
       "19763  79125         2\n",
       "19764  79129         2\n",
       "\n",
       "[19765 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(np.column_stack((ids, resps)), columns=['Id', 'Response'])\n",
    "sub.to_csv('sub.csv',index=False)\n",
    "pd.read_csv('sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
