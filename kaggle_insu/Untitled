import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
import snowballstemmer
import xgboost as xgb
from sklearn.cross_validation import train_test_split


df_train = pd.read_csv('train.csv', encoding="ISO-8859-1")
df_test = pd.read_csv('test.csv', encoding="ISO-8859-1")
df_pro_desc = pd.read_csv('product_descriptions.csv')

stemmer = snowballstemmer.EnglishStemmer()

def str_stemmer(s):
	return " ".join([stemmer.stemWord(wrd) for wrd in s.lower().split()])

def str_common_word(str1, str2):
	return sum(int(str2.find(word)>=0) for word in str1.split())

df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)
df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')


df_all['search_term'] = df_all['search_term'].map(lambda x:str_stemmer(x))
df_all['product_title'] = df_all['product_title'].map(lambda x:str_stemmer(x))
df_all['product_description'] = df_all['product_description'].map(lambda x:str_stemmer(x))
df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)
df_all['product_info'] = df_all['search_term']+"\t"+df_all['product_title']+"\t"+df_all['product_description']
df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\t')[0],x.split('\t')[1]))
df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\t')[0],x.split('\t')[2]))
df_all = df_all.drop(['search_term','product_title','product_description','product_info'],axis=1)

num_train = df_train.shape[0]

df_train = df_all.iloc[:num_train]
df_test = df_all.iloc[num_train:]
id_test = df_test['id']
y_train = df_train['relevance'].values
#X_train = df_train.drop(['id','relevance'],axis=1).values
#X_test = df_test.drop(['id','relevance'],axis=1).values

df_test.drop('relevance',axis=1,inplace=True)
df_test.drop('id',axis=1,inplace=True)
df_train.drop('relevance',axis=1,inplace=True)
df_train.drop('id',axis=1,inplace=True)


params = {}
params["objective"] ="reg:linear"
params["min_child_weight"] = 120
params["subsample"] = 1
params["colsample_bytree"] = .75
params["silent"] = 0
params["max_depth"] = 4
params["eta"]=0.15
params["gamma"]=1
plst=list(params.items())

x_train, x_test, y_train, y_test = train_test_split(df_train,y_train,test_size=0.33,random_state=0)

xdtest=xgb.DMatrix(x_test,y_test)
xdtrain=xgb.DMatrix(x_train,label=y_train)

bst=xgb.train(plst,xdtrain,num_boost_round=400)
test_preds=bst.predict(xdtest)

out=[]
for i in preds:
    closest= min(kk, key=lambda x:abs(x-i))
    out.append(closest)

    kimba2=[i-1 for i in kimba]

    le.inverse_transform(kimba2).max()

    from sklearn import preprocessing
    le = preprocessing.LabelEncoder()


    # In[25]:

    y_trainft=le.fit_transform(y_train)


    # In[81]:

    max(kimba)


    # In[46]:

    x_train, x_test, y_train, y_test = train_test_split(df_train,kimba,test_size=0.33,random_state=0)


    # In[47]:




    # In[83]:

    xd_test=xgb.DMatrix(df_test)
    xd_train=xgb.DMatrix(df_train,label=kimba)


    # In[67]:

    params = {}
    params["objective"] ="reg:linear"
    params["min_child_weight"] =240
    params["subsample"] = 1
    params["colsample_bytree"] = 0.75
    params["silent"] = 0
    params["max_depth"] = 4
    params["eta"]=0.1
    params["gamma"]=0
    plst=list(params.items())


    # In[68]:

    tbst=xgb.train(plst,xd_train,num_boost_round=400)
    preds=tbst.predict(xd_test)
    out=[]
    for i in preds:
        closest= min(kk, key=lambda x:abs(x-i))
        out.append(closest)
    print (mean_squared_error(out, y_test)**0.5)
    np.array(out).std(),y_test.std()


    # In[69]:

    pd.Series(out).unique()


    # In[91]:

    mean_squared_error(resps1, paka)**0.5


    # In[66]:

    paka=le.inverse_transform(y_test)


    # In[ ]:

    rf = RandomForestRegressor(n_estimators=15, max_depth=6, random_state=0)
    clf = BaggingRegressor(rf, n_estimators=45, max_samples=0.1, random_state=25)
    clf.fit(x_train, y_train)
    preds = clf.predict(x_test)
    out=[]
    for i in preds:
        closest= min(kk, key=lambda x:abs(x-i))
        out.append(closest)
    print (mean_squared_error(out, y_test)**0.5)
    np.array(out).std(),y_test.std()


    # In[ ]:

    kk


    # In[53]:

    kk=pd.Series(y_train).unique()


    # In[ ]:

    out=[]
    for i in ll:
        closest= min(kk, key=lambda x:abs(x-i))
        out.append(closest)


    # In[ ]:

    out


    # In[13]:

    from sklearn.metrics import mean_squared_error


    # In[ ]:

    print (mean_squared_error(out, y_test)**0.5)
    np.array(out).std(),y_test.std()


    # In[ ]:

    print (mean_squared_error(preds, y_test)**0.5)
    preds.std(),y_test.std()


    # In[52]:

    offsets = np.array([0.5]*13)


    # In[53]:

    len(offsets)


    # In[ ]:

    pd.Series(y_train).describe()


    # In[ ]:

    paka[16]


    # In[85]:

    tbst=xgb.train(plst,xd_train,num_boost_round=400)
    train_preds=tbst.predict(xd_train)#,ntree_limit=tbst.best_iteration)
    test_preds=tbst.predict(xd_test)#,ntree_limit=tbst.best_iteration)
    test_preds = np.clip(test_preds, -0.99, 13.99)
    train_preds = np.clip(train_preds, -0.99, 13.99)

    data1 = np.vstack((train_preds, train_preds,kimba))
    for j in range(13):
            data1[1, data1[0].astype(int)==j] = data1[0, data1[0].astype(int)==j] + offsets[j]
    for j in range(13):
            train_offset = lambda x: -apply_offset(data1, x, j)
            offsets[j] = fmin(train_offset, offsets[j])

    ll=data1[0].round().astype(int)
    k=pd.Series([0]*(len(df_test)))

    data = np.vstack((test_preds, test_preds, k))
    for j in range(13):
            data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j]

    resps = np.round(np.clip(data[1], 1, 13))


    # In[86]:

    resps1=le.inverse_transform([i-1 for i in resps.astype(int)])


    # In[68]:

    paka=le.inverse_transform([i-1 for i in y_test])


    # In[90]:

    resps1.std()


    # In[74]:

    mean_squared_error(resps1, paka)**0.5


    # In[72]:

    paka.std()


    # In[12]:

    from scipy.optimize import fmin


    # In[11]:

    def eval_wrapper(yhat, y):
            y = np.array(y)
            y = y.astype(int)
            yhat = np.array(yhat)
            yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)
            return quadratic_weighted_kappa(yhat, y)

    def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):
        # data has the format of pred=0, offset_pred=1, labels=2 in the first dim
        data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset
        score = scorer(data[1], data[2])
        return score


    # In[10]:

    from ml_metrics import quadratic_weighted_kappa


    # In[ ]:
